str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all("This is a backslash: \\ ", "")
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# a note on the stringi package
# source: [https://goo.gl/XzEQai]
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
######################
### IT'S YOUR SHOT ###
######################
## 1. describe the types of strings that conform to the following regular expressions and construct an example that is matched by the regular expression.
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
"\\b[a-z]{1,4}\\b"
".*?\\.txt$"
"\\d{2}/\\d{2}/\\d{4}"
"<(.+?)>.+?</\\1>"
## 2. consider the mail address  chunkylover53[at]aol[dot]com.
# a) transform the string to a standard mail format using regular expressions.
# b) imagine we are trying to extract the digits in the mail address using [:digit:]. explain why this fails and correct the expression.
email <- "chunkylover53[at]aol[dot]com"
## string manipulation ----------
# inspect text
x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all("This is a backslash: \ ", ""))
unlist(str_extract_all("This is a backslash: \ ", "\\"))
unlist(str_extract_all("This is a backslash: \ ", "\\\"))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# a note on the stringi package
# source: [https://goo.gl/XzEQai]
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
######################
### IT'S YOUR SHOT ###
######################
## 1. describe the types of strings that conform to the following regular expressions and construct an example that is matched by the regular expression.
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
"\\b[a-z]{1,4}\\b"
".*?\\.txt$"
"\\d{2}/\\d{2}/\\d{4}"
"<(.+?)>.+?</\\1>"
## 2. consider the mail address  chunkylover53[at]aol[dot]com.
# a) transform the string to a standard mail format using regular expressions.
# b) imagine we are trying to extract the digits in the mail address using [:digit:]. explain why this fails and correct the expression.
email <- "chunkylover53[at]aol[dot]com"
## string manipulation ----------
# inspect text
x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all("This is a backslash: \ ", "\\"))
unlist(str_extract_all("This is a backslash: \ ", fixed("\"))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# a note on the stringi package
# source: [https://goo.gl/XzEQai]
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
######################
### IT'S YOUR SHOT ###
######################
## 1. describe the types of strings that conform to the following regular expressions and construct an example that is matched by the regular expression.
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
"\\b[a-z]{1,4}\\b"
".*?\\.txt$"
"\\d{2}/\\d{2}/\\d{4}"
"<(.+?)>.+?</\\1>"
## 2. consider the mail address  chunkylover53[at]aol[dot]com.
# a) transform the string to a standard mail format using regular expressions.
# b) imagine we are trying to extract the digits in the mail address using [:digit:]. explain why this fails and correct the expression.
email <- "chunkylover53[at]aol[dot]com"
## string manipulation ----------
# inspect text
x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Blah )huge+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Blah )huge.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
str_extract_all("munich, pisa, firenze", "\\b[a-z]{1,4}\\b")
str_extract_all("I am looking for words of max length four", "\\b[a-z]{1,4}\\b")
str_extract_all(c("test.dta", "log.txt", "foo.txt "), ".*?\\.txt$")
str_extract_all(c("test.dta", "this is a log.txt", "foo.txt "), ".*?\\.txt$")
str_extract_all(c("18/05/2017", "18.05.2017"), "\\d{2}/\\d{2}/\\d{4}")
str_extract_all(c("18/5/2017", "18.05.2017"), "\\d{1,2}/\\d{1,2}/\\d{4}")
str_extract_all("<br><i>hello</i>", "<(.+?)>.+?</\\1>")
example.obj
str_locate(example.obj, "tiny")
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_locate(example.obj, "tiny")
str_sub(example.obj, start = 35, end = 38)
str_sub(example.obj, start = 33, end = 38)
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
str_replace(example.obj, pattern = "sentence", replacement = "house")
str_replace_all(example.obj, pattern = "sentence", replacement = "house")
example.obj
str_split(example.obj, "-") %>% unlist
str_split(example.obj, " ") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
(char.vec <- c("this", "and this", "and that"))
char.vec
?str_detect
str_detect(char.vec, "this")
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
str_dup(char.vec, 3)
length.char.vec <- str_length(char.vec)
length.char.vec
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
?str_trim
paste("text", "manipulation")
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
?agrep
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
?agrep
agrepl("Donald Trump", "Barack Obama", max.distance = list(all = 3))
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
browseURL("http://flukeout.github.io/") # let's play this together until plate 8 or so!
browseURL("https://www.jstatsoft.org/about/editorialTeam")
url <- "https://www.nytimes.com"
url <- "https://www.nytimes.com"
url_parsed <- read_html(url)
source("00-course-setup.r")
wd <- getwd()
url_parsed <- read_html(url)
class(url_parsed)
html_structure(url_parsed)
as_list(url_parsed)
?html_nodes
headings_nodes <- html_nodes(url_parsed, css = ".story-heading")
headings_nodes
headings <- html_text(headings_nodes)
headings
headings <- str_replace_all(headings, "\\n|\\t", "") %>% str_trim()
head(headings)
length(headings)
str_detect(headings, "Trump") %>% table()
url <- "https://www.nytimes.com"
url_parsed <- read_html(url)
headings_nodes <- html_nodes(url_parsed, css = ".story-heading")
headings <- html_text(headings_nodes)
read_html("https://www.nytimes.com") %>% html_nodes(css = ".story-heading") %>% html_text()
read_html("https://www.nytimes.com") %>% html_nodes(css = ".story-heading") %>% html_text() -> headlines
headlines
read_html(url) %>% html_nodes(".member") %>% html_text()
url <- "https://www.jstatsoft.org/about/editorialTeam"
read_html(url) %>% html_nodes(".member") %>% html_text()
read_html(url) %>% html_nodes(css = ".member")
read_html(url) %>% html_nodes(css = ".member") %>% html_text()
read_html(url) %>% html_nodes(css = ".member a") %>% html_text()
read_html(url) %>% html_nodes(css = ".editorialTeam a") %>% html_text()
read_html(url) %>% html_nodes("div.member") %>% html_text()
read_html(url) %>% html_nodes(css = "div.editorialTeam a") %>% html_text()
read_html(url) %>% html_nodes(css = "ol.editorialTeam a") %>% html_text()
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
browseURL(url)
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
tables <- html_table(url_parsed)
tables <- html_table(url_parsed, fill = TRUE)
tables
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
tables
meetings <- tables[[2]]
class(meetings)
head(meetings)
table(meetings$Location) %>% sort()
head(meetings)
browseURL("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
buildings <- tables[[6]]
View(buildings)
buildings$height <- buildings[,2]
buildings$height <- str_extract(buildings$height, "[[:digit:],.]+")
buildings$height <- str_extract(buildings$height, "[[:digit:],.]+") %>% str_replace(",", "")
class(buildings$height)
buildings$height <- str_extract(buildings$height, "[[:digit:],.]+") %>% str_replace(",", "") %>% as.numeric()
summary(buildings$height)
table(buildings$`Country/region`) %>% sort
names(buildings)
table(buildings$`Country`) %>% sort
table(buildings$City) %>% sort
table(buildings$`Country`) %>% sort
browseURL("http://selectorgadget.com/")
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text()
authors
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text() %>% str_replace_all("\\n", "") %>% str_trim()
table(authors) %>% sort
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
html <- read_html(url)
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
links <- links[!is.na(links)]
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
names <- links %>% basename %>% sapply(., URLdecode)  %>% str_replace_all("_", " ") %>% str_replace_all(" \\(.*\\)", "") %>% str_trim
names
links
basename(links)
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
## step 6: visualize connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
## step 7: identify top nodes in data frame
nodesDF$id <- as.numeric(rownames(nodesDF)) - 1
connections_df <- merge(connections, nodesDF, by.x = "to", by.y = "id", all = TRUE)
to_count_df <- count(connections_df, name)
arrange(to_count_df, desc(n))
