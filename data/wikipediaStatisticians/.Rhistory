browseURL("https://www.buzzfeed.com/?country=us")
devtools::install_github("ropensci/RSelenium")
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
content <- read_html(url)
source("00-course-setup.r")
content <- read_html(url)
checkForServer() ## deprecated; but gives advice on how to source/start a server
startServer()  ## deprecated
?rsDriver
rD <- rsDriver()
rD <- rsDriver()
?rsDriver
rD <- rsDriver(browser = "firefox")
?startServer
rd <-rsDriver(verbose =TRUE, browser = 'phantomjs', version = "3.2.0")
rd <-rsDriver(verbose =TRUE, browser = 'phantomjs', version = "3.4.0")
update.packages("RSelenium")
rD <- rsDriver(browser = "firefox")
vignette("RSelenium-docker", package = "RSelenium")
driver<- rsDriver()
install.packages("wdman")
install.packages("wdman")
library(RSelenium)
library(wdman)
selServ <- wdman::selenium(verbose = FALSE)
driver<- rsDriver()
cat(selServ)
selServ
driver<- rsDriver()
library(httr)
?GET
url <- "http://httpbin.org/get"
GET(url, add_headers(from = "eddie@datacollection.com"))
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
url_out
read_html(url_out)
write(url_out, "foo.html")
?write
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
write(url_out, file = "foo.html")
class(url_out)
content(url_out)
content(url_out) %>% write(file = "foo.html")
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
content(url_out) %>% write(file = "foo.html")
content(url_out)
content(url_out, as = "text") %>% write(file = "foo.html")
content(url_out, as = "text") %>% write(file = str_c("headlines-spiegel-", datetime, ".html"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("headlines-spiegel-", datetime, ".html"))
setwd("/Users/munzerts/github/rscraping-eui-2017")
launchctl stop spiegelheadlines
launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist
source("00-course-setup.r")
wd <- getwd()
tempwd <- ("data/breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
url <- "http://www.biermap24.de/brauereiliste.php"
browseURL(url)
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
length(cities)
length(unique(cities))
sort(table(cities))
# geocoding takes a while -> save results in local cache file
# 2500 requests allowed per day
if ( !file.exists("breweries_geo.RData")){
pos <- geocode(cities)
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
View(pos)
## step 3: plot breweries of Germany
brewery_map <- get_map(location=c(lon = mean(c(min(pos$lon), max(pos$lon))), lat = mean(c(min(pos$lat), max(pos$lat)))), zoom=6, maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=1)
p
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=.8)
p
setwd(wd)
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links
seq_along(links)[str_detect(links, "Odd_Aalen")]
class(links)
str_detect(links, "Odd_Aalen")
links[1:10]
links <- links[!is.na(links)]
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
## step 6: visualize connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
Fname
length(links)
links
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links <- links[!is.na(links)]
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
names
head(links)
head(names)
head()
head(anchors)
length(anchors)
links_index
links
names %>% basename %>% URLdecode()
names %>% basename
names <- links %>% basename %>% URLdecode()
names
names <- links %>% basename
names
names <- links %>% basename %>% sapply(., URLdecode)
names
names <- links %>% basename %>% sapply(., URLdecode) %>% str_replace(" \\(.*\\)", "")
names
names <- links %>% basename %>% sapply(., URLdecode) %>% str_replace_all(" \\(.*\\)", "")
names
names <- links %>% basename %>% sapply(., URLdecode) %>% str_replace_all(" \\(.*\\)", "") %>% str_replace_all("_", " ") %>% str_trim
names
"George Henry Wood (statistician)"  %>% str_replace_all(" \\(.*\\)", "")
names <- links %>% basename %>% sapply(., URLdecode)  %>% str_replace_all("_", " ") %>% str_replace_all(" \\(.*\\)", "") %>% str_trim
names
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
## step 2: retrieve links
html <- read_html(url)
anchors <- html_nodes(html, xpath = "//ul/li/a[1]")
links <- html_attr(anchors, "href")
links <- links[!is.na(links)]
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
##  step 3: extract names
names <- links %>% basename %>% sapply(., URLdecode)  %>% str_replace_all("_", " ") %>% str_replace_all(" \\(.*\\)", "") %>% str_trim
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between statisticians
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections <- connections[!duplicated(connections),]
## step 6: visualize connections
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
## step 7: identify top nodes in data frame
nodesDF$id <- as.numeric(rownames(nodesDF)) - 1
connections_df <- merge(connections, nodesDF, by.x = "to", by.y = "id", all = TRUE)
to_count_df <- count(connections_df, name)
arrange(to_count_df, desc(n))
