names <- html_nodes(url_parsed, css = ".member") %>% html_text()
names <- html_nodes(url_parsed, "#group a") %>% html_text()
xpath <- '//div[@id="content"]//li/a'
html_nodes(url_parsed, xpath = xpath) %>% html_text()
names
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
authors <- html_nodes(url_parsed, css = ".small-meta__item:nth-child(1) a") %>% html_text()
table(authors) %>% sort
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text()
table(authors) %>% sort
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text() %>% str_replace_all("\\n", "") %>% str_trim()
table(authors) %>% sort
browseURL("http://arxiv.org/help/api/index")
browseURL("http://export.arxiv.org/api/query?search_query=all:forecast")
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
library(aRxiv)
browseURL("http://ropensci.org/tutorials/arxiv_tutorial.html")
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 500, output_format = "data.frame")
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 200)
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20161110")
?article_pageviews
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user")
head(trump_views)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2017010100", end = "2017051500")
head(trump_views)
dim(trump_views)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "2017051500")
head(trump_views)
dim(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "2017051500")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
ymd_h(trump_views$timestamp)
names(trump_views)
plot(ymd_h(trump_views$date), trump_views$views, col = "red", type = "l")
trump_views$date
plot(trump_views$date, trump_views$views, col = "red", type = "l")
plot(trump_views$date, trump_views$views, col = "red", type = "l")
lines(clinton_views$date, clinton_views$views, col = "blue")
browseURL("http://openweathermap.org/current")
library(httr)
x <- GET("https://google.com")
x
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
browseURL("https://github.com/hrbrmstr/omdbapi")
browseURL("http://openweathermap.org/api")
credentials <- c(
"twitter_api_key=rN3T39JnSLKEBN9Pj7X2eBN",
"twitter_api_secret=abcqBpUzE7sa94jglkejgnJEnfzjyaRCfwn3ndrUUcqDWfhCN7Fj")
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
R.home()
openweathermap <- "42c7829f663f87eb05d2f12ab11f2b5d"
paste0(normalizePath("~/"),".Renviron")
?save
openweathermap <- "42c7829f663f87eb05d2f12ab11f2b5d"
save(openweathermap, file = paste0(normalizePath("~/"),"rkeys.RDa"))
save(openweathermap, file = paste0(normalizePath("~/"),"/rkeys.RDa"))
load(paste0(normalizePath("~/"),"/rkeys.Rda"))
apikey <- paste0("&appid=", openweathermap)
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Mannheim,Germany"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
source("00-course-setup.r")
wd <- getwd()
path.expand("~/")
load(paste0(path.expand("~/"), "twitter_token.rds")
## getting pageviews from Wikipedia ---------------------------
## IMPORTANT: If you want to gather pageviews data before July 2015, you need the statsgrokse package. Check it out here:
browseURL("https://github.com/cran/statsgrokse")
ls("package:pageviews")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2015070100", end = "2017040100")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2015070100", end = "2017040100")
plot(ymd(trump_views$date), trump_views$views, col = "red", type = "l")
lines(ymd(clinton_views$date), clinton_views$views, col = "blue")
german_parties_views <- article_pageviews(
project = "de.wikipedia",
article = c("Christlich Demokratische Union Deutschlands", "Christlich-Soziale Union in Bayern", "Sozialdemokratische Partei Deutschlands", "Freie Demokratische Partei", "Bündnis 90/Die Grünen", "Die Linke", "Alternative für Deutschland"),
user_type = "user",
start = "2015090100",
end = "2017040100"
)
load(paste0(path.expand("~/"), "twitter_token.rds"))
appname <- "rtweet_token"
key <- "xMOFcicpudZSCCfQpM6eA"
secret <- "4FrvNbK5MzxeRo9qTKjf3hMueICU8xBp9rVE1ySZ0k"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
library(rtweet)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
appname <- "rtweet_simon"
key <- "xMOFcicpudZSCCfQpM6eA"
secret <- "4FrvNbK5MzxeRo9qTKjf3hMueICU8xBp9rVE1ySZ0k"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
## name assigned to created app
appname <- "rtweet_simon"
## api key (example below is not a real key)
key <- "xMOFcicpudZSCCfQpM6eA"
## api secret (example below is not a real key)
secret <- "4FrvNbK5MzxeRo9qTKjf3hMueICU8xBp9rVE1ySZ0k"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
?create_token
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret,
cache = FALSE)
appname <- "TwitterToR"
key <- "uBoA6wJPiWXrcuTWUFbaTIApT"
secret <- "myhHWBOP3bPzd4oa38I9H5SesmAuUp4YZzgv97DtDxi4nlYlQM"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("data science", n = 1000, token = twitter_token)
View(rt)
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
q
q <- c("cdu","csu","spd","fdp","grüne","linke","afd","schulz","merkel","btw17")
q
streamtime <- 30 * 60
filename <- "rtelect.json"
?stream_tweets
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, file_name = "rtelect.json", token = twitter_token)
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
twitter_stream_ger
class(twitter_stream_ger)
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
View(twitter_stream_ger)
q <- paste0(c("cdu","csu","spd","fdp","grüne","linke","afd","schulz","merkel","btw17"), collapse = ",")
q
q <- paste0(c("cdu","csu","spd","fdp","grüne","linke","afd","schulz","merkel","btw17"), collapse = ",")
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
q
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
q <- paste0(c("cdu","csu","spd","fdp","grüne","linke","afd","schulz","merkel","btw17"), collapse = ",")
q
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
q <- paste0("trump,clinton,hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
twitter_stream_ger
class(twitter_stream_ger)
q <- paste0("trump,clinton,hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
q <- paste0("hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
twitter_stream_ger <- stream_tweets(q = q, timeout = 10, token = twitter_token)
class(twitter_stream_ger)
dim()
dim(twitter_stream_ger)
rtweet.folder <- path.expand("~/rtweet-data")
rtweet.folder
rtweet.folder <- paste0(getwd(),("data/rtweet-data")
## Create dedicated rtweet data directory
if (!dir.exists(rtweet.folder)) {
dir.create(rtweet.folder)
}
## Name streaming file based on time of stream
## Combine with directory to form path
filename <- file.path(
rtweet.folder, paste0(format(Sys.time(), "%F-%H-%S"), ".json"))
## create meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
## create file name for stream's meta data
metafile <- gsub(".json$", ".txt", filename)
## save meta data
cat(metadata, file = metafile)
## getting live data from Twitter ---------------------------
# how to interact with the streamR package
browseURL("http://pablobarbera.com/blog/archives/1.html")
# Stream keywords used to filter tweets
q <- c("cdu","csu","spd","fdp","grüne","linke","afd","schulz","merkel","btw17")
# load authentication credentials for Twitter. You have to have registered your app online and performed the OAuth authentication process to have this
load("twitter_auth.RData")
filterStream("german_parties.json", track = q, timeout = 60*1, oauth = twitCred)
tweets <- parseTweets("german_parties.json", simplify = TRUE)
names(tweets)
cat(tweets$text[1])
cdu <- str_detect(tweets$text, regex("cdu|csu|merkel", ignore_case = TRUE))
spd <- str_detect(tweets$text, regex("spd|schulz", ignore_case = TRUE))
fdp <- str_detect(tweets$text, regex("fdp", ignore_case = TRUE))
gru <- str_detect(tweets$text, regex("grüne", ignore_case = TRUE))
lin <- str_detect(tweets$text, regex("linke", ignore_case = TRUE))
afd <- str_detect(tweets$text, regex("afd", ignore_case = TRUE))
mentions_df <- data.frame(cdu, spd, fdp, gru, lin, afd)
colMeans(mentions_df)
load(paste0(path.expand("~/"), "twitter_token.rds"))
## getting pageviews from Wikipedia ---------------------------
## IMPORTANT: If you want to gather pageviews data before July 2015, you need the statsgrokse package. Check it out here:
browseURL("https://github.com/cran/statsgrokse")
ls("package:pageviews")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2015070100", end = "2017040100")
rtweet.folder <- paste0(getwd(),"data/rtweet-data")
rtweet.folder
## Create dedicated rtweet data directory
if (!dir.exists(rtweet.folder)) {
dir.create(rtweet.folder)
}
## Name streaming fi
dir.create("data/rtweet-data")
rtweet.folder <- "data/rtweet-data"
filename <- file.path(
rtweet.folder, paste0(format(Sys.time(), "%F-%H-%S"), ".json"))
filename
## create meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metadata
metafile <- gsub(".json$", ".txt", filename)
metafile
cat(metadata, file = metafile)
q <- paste0(c("schulz","merkel","btw17"), collapse = ",")
q
?stream_tweets
q
stream_tweets(q = q, parse = FALSE,
timeout = 10,
file_name = filename)
stream_tweets(q = q, parse = FALSE,
timeout = 10,
file_name = filename)
q <- paste0("love,hate")
stream_tweets(q = q, parse = FALSE,
timeout = 10,
file_name = filename)
stream_tweets(q = q, parse = FALSE,
timeout = 60,
file_name = filename)
rt <- parse_stream(filename)
View(rt)
q <- paste0("schulz,merkel,btw17,btw2017")
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "gerparties"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%S"), ".json"))
filename
## create meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metadata
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename)
rt <- parse_stream(filename)
head(rt)
names(rt)
users_data(rt) %>% head()
users_data(rt) %>% names()
q <- paste0("schulz,merkel,btw17,btw2017")
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "gerparties"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%S"), ".json"))
# create file with stream's meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 300,
file_name = filename)
q <- paste0("clinton,trump,hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "clintontrump"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%S"), ".json"))
# create file with stream's meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
stream_tweets(q = q, parse = FALSE,
timeout = 300,
file_name = filename)
q
filename
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%S"), ".json"))
filename
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%S"), ".json"))
filename
format(Sys.time(), "%F-%H-%S")
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 300,
file_name = filename)
rt <- parse_stream("clintontrump_2017-05-17-17-24-05.json")
rt <- parse_stream("data/rtweet-data/clintontrump_2017-05-17-17-24-05.json")
clinton <- str_detect(tweets$text, regex("hillary|clinton", ignore_case = TRUE))
names(rt)
clinton <- str_detect(rt$text, regex("hillary|clinton", ignore_case = TRUE))
trump <- str_detect(rt$text, regex("donald|trump", ignore_case = TRUE))
mentions_df <- data.frame(clinton,trump)
colMeans(mentions_df)
clinton
clinton <- str_detect(rt$text, regex("hillary|clinton", ignore_case = TRUE))
trump <- str_detect(rt$text, regex("donald|trump", ignore_case = TRUE))
mentions_df <- data.frame(clinton,trump)
mentions_df
colMeans(mentions_df, na.rm = TRUE)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2015070100", end = "2017040100")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2015070100", end = "2017040100")
plot(ymd(trump_views$date), trump_views$views, col = "red", type = "l")
lines(ymd(clinton_views$date), clinton_views$views, col = "blue")
german_parties_views <- article_pageviews(
project = "de.wikipedia",
article = c("Christlich Demokratische Union Deutschlands", "Christlich-Soziale Union in Bayern", "Sozialdemokratische Partei Deutschlands", "Freie Demokratische Partei", "Bündnis 90/Die Grünen", "Die Linke", "Alternative für Deutschland"),
user_type = "user",
start = "2015090100",
end = "2017040100"
)
table(german_parties_views$article)
parties <- unique(german_parties_views$article)
dat <- filter(german_parties_views, article == parties[1])
plot(ymd(dat$date), dat$views, col = "black", type = "l")
dat <- filter(german_parties_views, article == parties[2])
lines(ymd(dat$date), dat$views, col = "blue")
dat <- filter(german_parties_views, article == parties[3])
lines(ymd(dat$date), dat$views, col = "red")
dat <- filter(german_parties_views, article == parties[7])
lines(ymd(dat$date), dat$views, col = "brown")
library(gtrendsR)
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2016-10-01 2017-03-01")
gtrends_schulz <- gtrends("Schulz", geo = c("DE"), time = "2016-10-01 2017-03-01")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2016-10-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
openweathermap <- "42c7829f663f87eb05d2f12ab11f2b5d"
TwitterToR_twitterkey <- "uBoA6wJPiWXrcuTWUFbaTIApT"
TwitterToR_twittersecret <- "myhHWBOP3bPzd4oa38I9H5SesmAuUp4YZzgv97DtDxi4nlYlQM"
save(openweathermap,
TwitterToR_twitterkey,
TwitterToR_twittersecret,
file = paste0(normalizePath("~/"),"/rkeys.RDa"))
gtrends_schulz <- gtrends("Schulz", geo = c("DE"), time = "2017-01-01 2017-05-15")
plot(gtrends_merkel)
plot(gtrends_schulz)
?users_data
user_df <- lookup_users("RDataCollection")
user_df
user_friendships_df <- lookup_friendships("RDataCollection")
names()
names(user_df)
?lookup_friendships
user_friendships_df <- lookup_friendships("RDataCollection")
user_friendships_df <- lookup_friendships("RDataCollection")
user_friendships_df <- lookup_statuses("RDataCollection")
user_friendships_df
?lookup_statuses
user_timeline_df <- get_timeline("RDataCollection")
user_timeline_df
View(user_timeline_df)
user_favorites_df <- get_favorites("RDataCollection")
user_favorites_df
View(user_favorites_df)
?system
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
?session
source("00-course-setup.r")
wd <- getwd()
?session
?html_session
url <- "http://httpbin.org/user-agent"
html_session(url)
html_session(url) %>% read_html()
html_session(url) %>% class
html_session(url) %>% read_json()
-
url <- "http://httpbin.org/user-agent"
url <- "http://httpbin.org/user-agent"
html_session(url) %>% read_json()
url <- "http://httpbin.org/get"
html_session(url) %>% read_html
?html_session
html_session(url) %>% headers
GET("http://httpbin.org/user-agend", add_headers(user-agent = "foo"))
GET("http://httpbin.org/user-agent", add_headers(user-agent = "foo"))
GET("http://httpbin.org/user-agent", add_headers(useragent = "foo"))
GET("http://httpbin.org/user-agent", add_headers(From = "foo"))
GET("http://httpbin.org/headers", add_headers(From = "foo"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
url <- "http://spiegel.de/schlagzeilen"
html_session(url)
s <- html_session(url)
session <- html_session(url)
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
headlines
?session_info
session?html_session
?html_session
session_history()
session_history(session)
?html_session
session <- html_session(url, headers(From = "my@email.com"))
session <- html_session(url, add_headers(From = "my@email.com"))
session
url <- "http://httpbin.org/get"
session <- html_session(url, add_headers(From = "my@email.com"))
session %>% read_json
session
session %>% content %>% read_json
?content
session %>% content %>% fromJSON()
session %>%  fromJSON()
session %>%  content()
session %>%  response()
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://spiegel.de/schlagzeilen", add_headers(From = "my@email.com"))
url_response <- GET("http://spiegel.de/schlagzeilen", add_headers(From = "my@email.com"))
url_response %>% content()
url_parsed <- url_response  %>% read_html()
url_parsed
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
browseURL("http://httpbin.org")
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(`User-Agent` = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = "libcurl/7.51.0 r-curl/2.3 httr/1.2.1 bla"))
?GET
R.Version
R.Version()
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
session_history(session)
install.packages("robotstxt")
library(robotstxt)
paths_allowed("http://google.com/")
?paths_allowed
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
# remove everything in workspace
rm(list=ls(all=TRUE))
# load libraries
source("packages.r")
# load functions
source("functions.r")
# load turnout data
load("../data/national_level_data/national_turnout.RData")
head(national_turnout)
