x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all("This is a backslash: \ ", ""))
unlist(str_extract_all("This is a backslash: \ ", "\\"))
unlist(str_extract_all("This is a backslash: \ ", "\\\"))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# a note on the stringi package
# source: [https://goo.gl/XzEQai]
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
######################
### IT'S YOUR SHOT ###
######################
## 1. describe the types of strings that conform to the following regular expressions and construct an example that is matched by the regular expression.
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
"\\b[a-z]{1,4}\\b"
".*?\\.txt$"
"\\d{2}/\\d{2}/\\d{4}"
"<(.+?)>.+?</\\1>"
## 2. consider the mail address  chunkylover53[at]aol[dot]com.
# a) transform the string to a standard mail format using regular expressions.
# b) imagine we are trying to extract the digits in the mail address using [:digit:]. explain why this fails and correct the expression.
email <- "chunkylover53[at]aol[dot]com"
## string manipulation ----------
# inspect text
x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all("This is a backslash: \ ", "\\"))
unlist(str_extract_all("This is a backslash: \ ", fixed("\"))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
# do you think you can master regular expressions now?
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
# a note on the stringi package
# source: [https://goo.gl/XzEQai]
# stringr is built on top of the stringi package.
# stringr is convenient because it exposes a minimal set of functions, which have been carefully picked to handle the most common string manipulation functions.
# stringi is designed to be comprehensive. It contains almost every function you might ever need: stringi has 234 functions (compare that to stringr's 42)
# packages work very similarly; translating knowledge is easy (try stri_ instead of str_)
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
######################
### IT'S YOUR SHOT ###
######################
## 1. describe the types of strings that conform to the following regular expressions and construct an example that is matched by the regular expression.
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
"\\b[a-z]{1,4}\\b"
".*?\\.txt$"
"\\d{2}/\\d{2}/\\d{4}"
"<(.+?)>.+?</\\1>"
## 2. consider the mail address  chunkylover53[at]aol[dot]com.
# a) transform the string to a standard mail format using regular expressions.
# b) imagine we are trying to extract the digits in the mail address using [:digit:]. explain why this fails and correct the expression.
email <- "chunkylover53[at]aol[dot]com"
## string manipulation ----------
# inspect text
x <- stri_rand_lipsum(1)
str_view(x, "et")
str_view_all(x, "et")
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Blah )huge+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Blah )huge.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
str_extract_all("munich, pisa, firenze", "\\b[a-z]{1,4}\\b")
str_extract_all("I am looking for words of max length four", "\\b[a-z]{1,4}\\b")
str_extract_all(c("test.dta", "log.txt", "foo.txt "), ".*?\\.txt$")
str_extract_all(c("test.dta", "this is a log.txt", "foo.txt "), ".*?\\.txt$")
str_extract_all(c("18/05/2017", "18.05.2017"), "\\d{2}/\\d{2}/\\d{4}")
str_extract_all(c("18/5/2017", "18.05.2017"), "\\d{1,2}/\\d{1,2}/\\d{4}")
str_extract_all("<br><i>hello</i>", "<(.+?)>.+?</\\1>")
example.obj
str_locate(example.obj, "tiny")
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_locate(example.obj, "tiny")
str_sub(example.obj, start = 35, end = 38)
str_sub(example.obj, start = 33, end = 38)
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
str_replace(example.obj, pattern = "sentence", replacement = "house")
str_replace_all(example.obj, pattern = "sentence", replacement = "house")
example.obj
str_split(example.obj, "-") %>% unlist
str_split(example.obj, " ") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
(char.vec <- c("this", "and this", "and that"))
char.vec
?str_detect
str_detect(char.vec, "this")
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
str_dup(char.vec, 3)
length.char.vec <- str_length(char.vec)
length.char.vec
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
?str_trim
paste("text", "manipulation")
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
?agrep
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
?agrep
agrepl("Donald Trump", "Barack Obama", max.distance = list(all = 3))
?stri_count_words
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[firenze]")
?remoteDriver
source("00-course-setup.r")
?remoteDriver
remDr <- remoteDriver$new()
remDr$open()
startServer()
driver<- rsDriver()
?rsDriver
?rsDriver
rD <- rsDriver()
system("java -jar selenium-server-standalone-3.4.0.jar")
system("java -jar selenium-server-standalone-3.1.0.jar")
system("java -jar selenium-server-standalone-2.53.1.jar")
system("java -version")
system("java -jar selenium-server-standalone-3.4.0.jar")
system("dir")
system("ls")
browseURL("http://ropensci.org/")
browseURL("http://arxiv.org/help/api/index")
load("/Users/munzerts/rkeys.RDa")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
source("00-course-setup.r")
wd <- getwd()
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
browseURL("https://www.google.com/robots.txt")
browseURL("http://www.nytimes.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
Sys.time()
str_replace_all(Sys.time(), "[ :]", "-")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl list")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("http://arxiv.org/help/api/index")
browseURL("http://arxiv.org/help/api/user-manual")
browseURL("http://export.arxiv.org/api/query?search_query=all:forecast")
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
source("00-course-setup.r")
wd <- getwd()
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
class(read_xml)
class(forecast)
forecast
?xml_find_all
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors
authors %>% xml_text()
browseURL("http://ropensci.org/tutorials/arxiv_tutorial.html")
library(aRxiv)
arxiv_search
?arxiv_search
query_terms
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 200, output_format = "data.frame")
View(arxiv_df)
arxiv_count('au:"Gary King"')
query_terms
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "2017051500")
head(trump_views)
View(trump_views)
?article_pageviews
plot(trump_views$date, trump_views$views, col = "red", type = "l")
lines(clinton_views$date, clinton_views$views, col = "blue")
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "2017051500")
lines(clinton_views$date, clinton_views$views, col = "blue")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Elections_in_Egypt", user_type = "user", start = "2016010100", end = "2017051500")
trump_views
trump_views <- article_pageviews(project = "en.wikipedia", article = "Elections in Egypt", user_type = "user", start = "2016010100", end = "2017051500")
trump_views
?article_pageviews
browseURL("https://github.com/ropensci/opendata")
browseURL("http://www.omdbapi.com/")
title <- "Groundhog Day" %>% URLencode()
title
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
url
omdb_res = GET(url)
omdb_res
res_list <- content(omdb_res, as =  "parsed")
res_list
res_list %>% unlist()
res_list %>% unlist() %>% t()
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
browseURL("https://github.com/hrbrmstr/omdbapi")
browseURL("http://openweathermap.org/api")
browseURL("http://openweathermap.org/api")
apikey <- "&appid=42c7829f663f87eb05d2f12ab11f2b5d"
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Firenze, Italy"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
weather_res
res_list <- content(weather_res, as =  "parsed")
res_list
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
load("/Users/munzerts/rkeys.RDa")
load("/Users/munzerts/rkeys.RDa")
apikey <- paste0("&appid=", openweathermap)
apikey
appname <- "TwitterToR"
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
source("00-course-setup.r")
wd <- getwd()
## name assigned to created app
appname <- "TwitterToR"
## api key (example below is not a real key)
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
## api secret (example below is not a real key)
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
library(rtweet)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
twitter_token
?search_tweets
rt <- search_tweets("data science", n = 200, token = twitter_token)
View(rt)
q <- paste0("clinton,trump,hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
View(twitter_stream_ger)
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "clintontrump"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename
# create file with stream's meta data
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
streamtime <- format(Sys.time(), "%F-%H-%M-%S"), ".json")
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
metadata
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename)
rt <- parse_stream(filename)
View(rt)
names(rt)
users_data(rt) %>% head()
users_data(rt) %>% names()
rt <- parse_stream("data/rtweet-data/clintontrump_2017-05-19-16-27-32.json")
clinton <- str_detect(rt$text, regex("hillary|clinton", ignore_case = TRUE))
trump <- str_detect(rt$text, regex("donald|trump", ignore_case = TRUE))
mentions_df <- data.frame(clinton,trump)
head(mentions_df)
colMeans(mentions_df, na.rm = TRUE)
user_df <- lookup_users("RDataCollection")
names(user_df)
user_timeline_df <- get_timeline("RDataCollection")
names(user_timeline_df)
dim(user_timeline_df)
View(user_timeline_df)
user_favorites_df <- get_favorites("RDataCollection")
names(user_favorites_df)
devtools::install_github('PMassicotte/gtrendsR')
library(gtrendsR)
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
library(gtrendsR)
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_schulz <- gtrends("Schulz", geo = c("DE"), time = "2017-01-01 2017-05-15")
library(gtrendsR)
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_schulz <- gtrends("Schulz", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_merkel <- gtrends("Merkel", geo = c("DE"), time = "2017-01-01 2017-05-15")
gtrends_schulz <- gtrends("Schulz", geo = c("DE"), time = "2017-01-01 2017-05-15")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
