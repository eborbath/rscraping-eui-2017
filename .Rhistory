browseURL("https://www.buzzfeed.com/?country=us")
devtools::install_github("ropensci/RSelenium")
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
content <- read_html(url)
source("00-course-setup.r")
content <- read_html(url)
checkForServer() ## deprecated; but gives advice on how to source/start a server
startServer()  ## deprecated
?rsDriver
rD <- rsDriver()
rD <- rsDriver()
?rsDriver
rD <- rsDriver(browser = "firefox")
?startServer
rd <-rsDriver(verbose =TRUE, browser = 'phantomjs', version = "3.2.0")
rd <-rsDriver(verbose =TRUE, browser = 'phantomjs', version = "3.4.0")
update.packages("RSelenium")
rD <- rsDriver(browser = "firefox")
vignette("RSelenium-docker", package = "RSelenium")
driver<- rsDriver()
install.packages("wdman")
install.packages("wdman")
library(RSelenium)
library(wdman)
selServ <- wdman::selenium(verbose = FALSE)
driver<- rsDriver()
cat(selServ)
selServ
driver<- rsDriver()
library(httr)
?GET
url <- "http://httpbin.org/get"
GET(url, add_headers(from = "eddie@datacollection.com"))
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
url_out
read_html(url_out)
write(url_out, "foo.html")
?write
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
write(url_out, file = "foo.html")
class(url_out)
content(url_out)
content(url_out) %>% write(file = "foo.html")
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
content(url_out) %>% write(file = "foo.html")
content(url_out)
content(url_out, as = "text") %>% write(file = "foo.html")
content(url_out, as = "text") %>% write(file = str_c("headlines-spiegel-", datetime, ".html"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("headlines-spiegel-", datetime, ".html"))
setwd("/Users/munzerts/github/rscraping-eui-2017")
launchctl stop spiegelheadlines
launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist
source("00-course-setup.r")
?html_tag
?html_name
?html_form
?html_session
url <- "http://quipqiup.com/"
url_parsed <- read_html(url)
html_form(url_parsed)
url_parsed
url <- "http://earthquaketrack.com/"
url_parsed <- read_html(url)
html_form(url_parsed)
earthquake <- html_form(url_parsed)
html_form(url_parsed)
earthquake <- html_form(url_parsed)[[1]]
earthquake_form <- set_values(earthquake, q = "Florence, Italy)
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
### glossary: rvest's main functions --------------
read_html()
html_nodes()
html_text()
html_attr()
html_attrs()
read_xml()
xml_nodes()
xml_text()
xml_attr()
xml_attrs()
html_table()
html_form()
set_values()
submit_form()
guess_encoding()
repair_encoding()
html_session()
jump_to()
follow_link()
back()
forward()
earthquake_form <- set_values(earthquake, q = "Florence, Italy")
html_form(url_parsed)
earthquake <- html_form(url_parsed)[[1]]
earthquake_form <- set_values(earthquake, q = "Florence, Italy")
session <- html_session(url, user_agent(uastring))
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
earthquake_search <- submit_form(session, earthquake_form)
url_parsed <- read_html(earthquake_search)
html_table(url_parsed)
url_parsed %>% html_nodes("li a")
earthquake_search <- submit_form(session, earthquake_form)
url_parsed <- read_html(earthquake_search)
url_parsed %>% html_nodes("li a")
session
earthquake_search <- submit_form(session, earthquake_form)
earthquake_search
url_parsed <- read_html(earthquake_search)
url_parsed
cat(url_parsed)
html_structure(url_parsed)
as_list(url_parsed)
url_parsed %>% html_nodes("a")
url_parsed %>% html_nodes("li a")
url_parsed %>% html_nodes(".url a")
url_parsed %>% html_nodes(".url a") %>% html_attr("href")
?follow_link
url_parsed %>% html_nodes(".url a") %>% html_attr("href") %>% jump_to()
url_link <- url_parsed %>% html_nodes(".url a") %>% html_attr("href")
session %>% follow_link(url_link
### glossary: rvest's main functions --------------
read_html()
html_nodes()
html_text()
html_attr()
html_attrs()
read_xml()
xml_nodes()
xml_text()
xml_attr()
xml_attrs()
html_table()
html_form()
set_values()
submit_form()
guess_encoding()
repair_encoding()
html_session()
jump_to()
follow_link()
back()
forward()
session %>% follow_link(url_link)
url_link
read_html(url_link)
url_parsed <- read_html(url_link)
html_table(url_parsed)
url_parsed <- read_html(url_link)
html_table(url_parsed)
url_parsed
url_parsed <- read_html(url_link) %>% html_nodes(".col-sm-4 li")
url_parsed
url_parsed <- read_html(url_link) %>% html_nodes(".col-sm-4 li") %>% html_text()
url_parsed
?follow_link
?back
session_history
session_history(session)
url <- "https://www.jstatsoft.org/about/editorialTeam"
url_parsed <- read_html(url)
names <- html_nodes(url_parsed, css = ".member a") %>% html_text()
names <- html_nodes(url_parsed, css = ".member") %>% html_text()
names <- html_nodes(url_parsed, "#group a") %>% html_text()
xpath <- '//div[@id="content"]//li/a'
html_nodes(url_parsed, xpath = xpath) %>% html_text()
names
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
authors <- html_nodes(url_parsed, css = ".small-meta__item:nth-child(1) a") %>% html_text()
table(authors) %>% sort
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text()
table(authors) %>% sort
authors <- html_nodes(url_parsed, css = ".link-gray-lighter .xs-text-6") %>% html_text() %>% str_replace_all("\\n", "") %>% str_trim()
table(authors) %>% sort
browseURL("http://arxiv.org/help/api/index")
browseURL("http://export.arxiv.org/api/query?search_query=all:forecast")
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
library(aRxiv)
browseURL("http://ropensci.org/tutorials/arxiv_tutorial.html")
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 500, output_format = "data.frame")
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 200)
library(pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20161110")
?article_pageviews
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user")
head(trump_views)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2017010100", end = "2017051500")
head(trump_views)
dim(trump_views)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "2017051500")
head(trump_views)
dim(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "2017051500")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
ymd_h(trump_views$timestamp)
names(trump_views)
plot(ymd_h(trump_views$date), trump_views$views, col = "red", type = "l")
trump_views$date
plot(trump_views$date, trump_views$views, col = "red", type = "l")
plot(trump_views$date, trump_views$views, col = "red", type = "l")
lines(clinton_views$date, clinton_views$views, col = "blue")
browseURL("http://openweathermap.org/current")
library(httr)
x <- GET("https://google.com")
x
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
browseURL("https://github.com/hrbrmstr/omdbapi")
browseURL("http://openweathermap.org/api")
credentials <- c(
"twitter_api_key=rN3T39JnSLKEBN9Pj7X2eBN",
"twitter_api_secret=abcqBpUzE7sa94jglkejgnJEnfzjyaRCfwn3ndrUUcqDWfhCN7Fj")
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
R.home()
openweathermap <- "42c7829f663f87eb05d2f12ab11f2b5d"
paste0(normalizePath("~/"),".Renviron")
?save
openweathermap <- "42c7829f663f87eb05d2f12ab11f2b5d"
save(openweathermap, file = paste0(normalizePath("~/"),"rkeys.RDa"))
save(openweathermap, file = paste0(normalizePath("~/"),"/rkeys.RDa"))
load(paste0(normalizePath("~/"),"/rkeys.Rda"))
apikey <- paste0("&appid=", openweathermap)
endpoint <- "http://api.openweathermap.org/data/2.5/find?"
city <- "Mannheim,Germany"
metric <- "&units=metric"
url <- paste0(endpoint, "q=", city, metric, apikey)
weather_res <- GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list
res_list <- content(weather_res, as =  "text")  %>% jsonlite::fromJSON(flatten = TRUE)
res_list$list
