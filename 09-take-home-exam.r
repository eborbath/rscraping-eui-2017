### -----------------------------
### simon munzert
### take-home exam
### -----------------------------



## 1. A Network of Political Scientists ------------

# The English Wikipedia features an entry with a list of political scientists around the world: https://en.wikipedia.org/wiki/List_of_political_scientists. Make use of this list to (1) download all articles of the listed scientists to your hard drive, (2) gather the network structure behind these articles and visualize it, and (3) identify the top 10 of political scientists that have the most links from other political scientists pointing to their page!


## 2. Cracking a Secret Message --------------------

# The following code hides a secret message. Crack it with R and regular expressions.

secret <- "clcopCow1zmstc0d87wnkig7OvdicpNuggvhryn92Gjuwczi8hqrfpRxs5Aj5dwpn0Tanwo Uwisdij7Lj8kpf03AT5Idr3coc0bt7yczjatOaootj55t3Nj3ne6c4Sfek.r1w1YwwojigO d6vrfUrbz2.2bkAnbhzgv4R9i05zEcrop.wAgnb.SqoU65fPa1otfb7wEm24k6t3sR9zqe5 fy89n6Nd5t9kc4fE905gmc4Rgxo5nhDk!gr"


## 3. Cognitive Biases ------------------------------

# The Wikipedia article at http://en.wikipedia.org/wiki/List_of_cognitive_biases provides several lists of various types of cognitive biases.
# 3.1 Parse the page!
# 3.2 Extract the information stored in the table on social biases and store it in an R data frame!
# 3.3 Fetch all footnotes on the page from the "Notes" section and parse it into a table, separating authors, title of the reference/article and year, if possible!
# 3.4 Each of the entries in the table on social biases points to another, more detailed article on Wikipedia. First, fetch all the links. Next, download all the articles behind the links. Finally, parse all pages into R and make a wordcloud of all the data (the wordcloud package could be of help).


## 4. And Now for the Weather ------------------------------

## Yahoo! provides a free weather RSS feed that enables you to get up-to-date weather information for any location. 
# 4.1 Explore the API at https://developer.yahoo.com/weather/documentation.html!
# 4.2 The API requires a WOEID to identify the location. Try to find a source on the Web that returns a WOEID for a written location name (e.g., 'Berlin').
# 4.3 Access the API with R and transform the data into a useful format, e.g., a data.frame object!


## 5. Your Own Scraping Project ----------------------------

## Now you are prepared to get started with your own scraping project! Document a scraping effort that features the following characteristics:

  # scrape data from more than one URL (e.g., several HTML pages from one webpage)
  # process the data in a way that allows for meaningful analyses (e.g., tidy it and parse it into a clean data.frame)
  # explore the data using visuals and/or descriptive statistics




